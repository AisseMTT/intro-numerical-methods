{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"./images/CC-BY.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli</td>\n",
    "</table>\n",
    "\n",
    "Note:  The presentation below largely follows part II in \"Finite Difference Methods for Ordinary and Partial Differential Equations\" by LeVeque (SIAM, 2007)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Solution to ODE Initial Value Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many physical, biological, and societal systems can be written as a system of ordinary differential equations (ODEs).  In the case where the initial state (value) is know the problems can be written as\n",
    "\n",
    "$$\\frac{\\text{d} \\vec{u}}{\\text{d}t} = \\vec{f}(t, \\vec{u}) ~~~~ \\vec{u}(0) = \\vec{u}_0$$\n",
    "\n",
    "where\n",
    " - $\\vec{u}(t)$ is the state vector\n",
    " - $\\vec{f}(t, \\vec{u})$ is a vector-valued function that controls the growth of $\\vec{u}$ with time\n",
    " - $\\vec{u}(0)$ is the initial condition at time $t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples:  Simple radioactive decay\n",
    "$\\vec{u} = [c]$\n",
    "   \n",
    "$$\\frac{\\text{d} c}{\\text{d}t} = -\\lambda c ~~~~ c(0) = c_0$$\n",
    "   \n",
    "\n",
    "which has solutions of the form $c(t) = c_0 e^{-\\lambda t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "t = numpy.linspace(0.0, 1.6e3, 100)\n",
    "c_0 = 1.0\n",
    "decay_constant = numpy.log(2.0) / 1600.0\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(t, 1.0 * numpy.exp(-decay_constant * t))\n",
    "\n",
    "axes.set_title(\"Radioactive Decay with $t_{1/2} = 1600$ years\")\n",
    "axes.set_xlabel('t (years)')\n",
    "axes.set_ylabel('$c$')\n",
    "axes.set_ylim((0.5,1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples:  Complex radioactive decay (or chemical system).\n",
    "\n",
    "Chain of decays from one species to another.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\text{d} c_1}{\\text{d}t} &= -\\lambda_1 c_1 \\\\\n",
    "    \\frac{\\text{d} c_2}{\\text{d}t} &= \\lambda_1 c_1 - \\lambda_2 c_2 \\\\\n",
    "    \\frac{\\text{d} c_2}{\\text{d}t} &= \\lambda_2 c_3 - \\lambda_3 c_3 \n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\frac{\\text{d} \\vec{u}}{\\text{d}t} = \\frac{\\text{d}}{\\text{d}t}\\begin{bmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "    -\\lambda_1 & 0 & 0 \\\\\n",
    "    \\lambda_1 & -\\lambda_2 & 0 \\\\\n",
    "    0 & \\lambda_2 & -\\lambda_3\n",
    "\\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix}$$\n",
    "\n",
    "$$\\frac{\\text{d} \\vec{u}}{\\text{d}t} = A \\vec{u}$$\n",
    "\n",
    "For systems of equations like this the general solution to the ODE is the matrix exponential:\n",
    "\n",
    "$$\\vec{u}(t) = \\vec{u}_0 e^{A t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples: Van der Pol Oscillator\n",
    "\n",
    "$$y'' - \\mu (1 - y^2) y' + y = 0~~~~~\\text{with}~~~~ y(0) = y_0, ~~~y'(0) = v_0$$\n",
    " \n",
    "$$\\vec{u} = \\begin{bmatrix} y \\\\ y' \\end{bmatrix} = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix}$$\n",
    "   \n",
    "$$\\frac{\\text{d}}{\\text{d}t} \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} u_2 \\\\ \\mu (1 - u_1^2) u_2 - u_1 \\end{bmatrix} = \\vec{f}(t, \\vec{u})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code demo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "\n",
    "def f(t, u, mu=5):\n",
    "    return numpy.array([u[1], mu * (1.0 - u[0]**2) * u[1] - u[0]])\n",
    "\n",
    "t = numpy.linspace(0.0, 100, 500)\n",
    "u = numpy.empty((2, 500))\n",
    "u[:, 0] = [0.1, 0.0]\n",
    "\n",
    "integrator = integrate.ode(f)\n",
    "integrator.set_integrator(\"dopri5\")\n",
    "integrator.set_initial_value(u[:, 0])\n",
    "\n",
    "for (n, t_n) in enumerate(t[1:]):\n",
    "    integrator.integrate(t_n)\n",
    "    if not integrator.successful():\n",
    "        break\n",
    "    u[:, n + 1] = integrator.y\n",
    "    \n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.plot(t, u[0,:])\n",
    "axes.set_title(\"Solution to Van der Pol Oscillator\")\n",
    "axes.set_xlabel(\"t\")\n",
    "axes.set_ylabel(\"y(t)\")\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.plot(u[0,:], u[1, :])\n",
    "axes.set_title(\"Phase Diagram for Van der Pol Oscillator\")\n",
    "axes.set_xlabel(\"y(t)\")\n",
    "axes.set_ylabel(\"y'(t)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples:  Particle tracking in a fluid\n",
    "\n",
    "$$\\frac{\\text{d} \\vec{X}}{\\text{d}t} = \\vec{V}(t, \\vec{X})$$\n",
    "\n",
    "In fact all ODE IVP systems can be thought of as tracking particles through a flow field (dynamical system).  In 1-dimension the flow \"manifold\" we are on is fixed by the initial condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Stepping Schemes\n",
    "\n",
    "Introducing some notation to simpify things\n",
    "$$\\begin{aligned}\n",
    "    t_0 &= 0 \\\\\n",
    "    t_1 &= t_0 + \\Delta t \\\\\n",
    "    t_n &= t_{n-1} + \\Delta t = n \\Delta t + t_0 \\\\\n",
    "    u_0 &= u(t_0) \\\\\n",
    "    u_1 &= u(t_1) \\\\\n",
    "    u_n &= u(t_n) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looking back at our work on numerical differentiation why not approximate the derivative as a finite difference:\n",
    "$$\n",
    "    \\frac{u(t + \\Delta t) - u(t)}{\\Delta t} = f(t, u)\n",
    "$$\n",
    "\n",
    "We still need to decide how to evaluate the $f(t, u)$ term however.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us look at this from a perspective of quadrature, take the intergral of both sides:\n",
    "$$\\begin{aligned}\n",
    "    \\int^{t + \\Delta t}_t \\frac{\\text{d} u}{\\text{d}\\tilde{t}} d\\tilde{t} &= \\int^{t + \\Delta t}_t f(t, u) d\\tilde{t} \\\\\n",
    "    u(t + \\Delta t) - u(t) &\\approx \\Delta t f(t, u(t))\n",
    "\\end{aligned}$$\n",
    "where we have used a left-sided quadrature rule for the integral on the right.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can rewrite our scheme\n",
    "$$\n",
    "    u(t + \\Delta t) - u(t) = \\Delta t f(t, u(t))\n",
    "$$\n",
    "as\n",
    "$$\n",
    "    \\frac{u_{n+1} - u_n}{\\Delta t} = f(t_n, u_n)\n",
    "$$\n",
    "or\n",
    "$$\n",
    "    u_{n+1} = u_n + \\Delta t f(t_n, u_n)\n",
    "$$\n",
    "which is known as the *forward Euler method*.  In essence we are approximating the derivative with the value of the function at the point we are at $t_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "t = numpy.linspace(0.0, 1.6e3, 100)\n",
    "c_0 = 1.0\n",
    "decay_constant = numpy.log(2.0) / 1600.0\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(t, c_0 * numpy.exp(-decay_constant * t), label=\"True Solution\")\n",
    "\n",
    "# Plot Euler step\n",
    "dt = 1e3\n",
    "u_np = c_0 + dt * (-decay_constant * c_0)\n",
    "axes.plot((0.0, dt), (c_0, u_np), 'k')\n",
    "axes.plot((dt, dt), (u_np, c_0 * numpy.exp(-decay_constant * dt)), 'k--')\n",
    "axes.plot((0.0, 0.0), (c_0, u_np), 'k--')\n",
    "axes.plot((0.0, dt), (u_np, u_np), 'k--')\n",
    "axes.text(400, u_np - 0.05, '$\\Delta t$', fontsize=16)\n",
    "\n",
    "axes.set_title(\"Radioactive Decay with $t_{1/2} = 1600$ years\")\n",
    "axes.set_xlabel('t (years)')\n",
    "axes.set_ylabel('$c$')\n",
    "axes.set_xlim(-1e2, 1.6e3)\n",
    "axes.set_ylim((0.5,1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A similar method can be derived if we consider instead using the second order accurate central difference:\n",
    "\n",
    "$$\\frac{u_{n+1} - u_{n-1}}{2\\Delta t} = f(t_{n}, u_{n})$$\n",
    "\n",
    "this method is known as the leap-frog method.  Note that the way we have written this method requires a previous function evaluation and technically is a \"multi-step\" method although we do not actually use the current evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code demo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "t = numpy.linspace(0.0, 1.6e3, 100)\n",
    "c_0 = 1.0\n",
    "decay_constant = numpy.log(2.0) / 1600.0\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(t, c_0 * numpy.exp(-decay_constant * t), label=\"True Solution\")\n",
    "\n",
    "# Plot Leap-Frog step\n",
    "dt = 1e3\n",
    "u_np = c_0 + dt * (-decay_constant * c_0 * numpy.exp(-decay_constant * dt / 2.0))\n",
    "axes.plot((0.0, dt), (c_0, u_np), 'k')\n",
    "axes.plot((dt, dt), (u_np, c_0 * numpy.exp(-decay_constant * dt)), 'k--')\n",
    "axes.plot((0.0, 0.0), (c_0, u_np), 'k--')\n",
    "axes.plot((0.0, dt), (u_np, u_np), 'k--')\n",
    "axes.text(400, u_np - 0.05, '$\\Delta t$', fontsize=16)\n",
    "\n",
    "axes.set_title(\"Radioactive Decay with $t_{1/2} = 1600$ years\")\n",
    "axes.set_xlabel('t (years)')\n",
    "axes.set_ylabel('$c$')\n",
    "axes.set_xlim(-1e2, 1.6e3)\n",
    "axes.set_ylim((0.5,1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "c_0 = 1.0\n",
    "decay_constant = numpy.log(2.0) / 1600.0\n",
    "f = lambda t, u: -decay_constant * u\n",
    "\n",
    "t_exact = numpy.linspace(0.0, 1.6e3, 100)\n",
    "u_exact = c_0 * numpy.exp(-decay_constant * t_exact)\n",
    "\n",
    "# Implement Euler\n",
    "t_euler = numpy.linspace(0.0, 1.6e3, 10)\n",
    "delta_t = t_euler[1] - t_euler[0]\n",
    "u_euler = numpy.empty(t_euler.shape)\n",
    "u_euler[0] = c_0\n",
    "for (n, t_n) in enumerate(t_euler[:-1]):\n",
    "    u_euler[n + 1] = u_euler[n] + delta_t * f(t_n, u_euler[n])\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(t_euler, u_euler, 'or', label=\"Euler\")\n",
    "axes.plot(t_exact, u_exact, 'k--', label=\"True Solution\")\n",
    "\n",
    "axes.set_title(\"Forward Euler\")\n",
    "axes.set_xlabel(\"t (years)\")\n",
    "axes.set_xlabel(\"$c(t)$\")\n",
    "axes.set_ylim((0.4,1.1))\n",
    "axes.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "c_0 = 1.0\n",
    "decay_constant = numpy.log(2.0) / 1600.0\n",
    "f = lambda t, u: -decay_constant * u\n",
    "\n",
    "t_exact = numpy.linspace(0.0, 1.6e3, 100)\n",
    "u_exact = c_0 * numpy.exp(-decay_constant * t_exact)\n",
    "\n",
    "# Implement leap-frog\n",
    "t_leapfrog = numpy.linspace(0.0, 1.6e3, 10)\n",
    "delta_t = t_leapfrog[1] - t_leapfrog[0]\n",
    "u_leapfrog = numpy.empty(t_leapfrog.shape)\n",
    "u_leapfrog[0] = c_0\n",
    "# First evaluation use Euler to get us going\n",
    "u_leapfrog[1] = u_leapfrog[0] + delta_t * f(t_leapfrog[0], u_leapfrog[0])\n",
    "for n in xrange(1, t_leapfrog.shape[0] - 1):\n",
    "    u_leapfrog[n + 1] = u_leapfrog[n - 1] + 2.0 * delta_t * f(t[n], u_leapfrog[n])\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(t_leapfrog, u_leapfrog, 'or', label=\"Leap-Frog\")\n",
    "axes.plot(t_exact, u_exact, 'k--', label=\"True Solution\")\n",
    "\n",
    "axes.set_title(\"Leap-Frog\")\n",
    "axes.set_xlabel(\"t (years)\")\n",
    "axes.set_xlabel(\"$c(t)$\")\n",
    "axes.set_ylim((0.4,1.1))\n",
    "axes.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error Analysis of ODE Methods\n",
    "\n",
    "At this point it is also helpful to introduce more notation to distinguish between the true solution to the ODE $u(t_n)$ and the approximated value which we will denote $U_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition:** We define the *truncation error* of a scheme by\n",
    "\n",
    "$$T(t, u; \\Delta t) = \\frac{1}{\\Delta t} [U_{n+1} - u(t + \\Delta t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition:** A method is called *consistent* if \n",
    "$$\n",
    "    \\lim_{\\Delta t \\rightarrow 0} T(t, u; \\Delta t) = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition:** We say that a method is *order* $p$ accurate if\n",
    "\n",
    "$$||T(t, u; \\Delta t) || \\leq C \\Delta t^p$$\n",
    "\n",
    "uniformally on $t \\in [0, T]$.  This can also be written as $T(t, u; \\Delta t) = \\mathcal{O}(\\Delta t^p)$.  Note that a method is consistent if $p > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error Analysis of Forward Euler\n",
    "\n",
    "We can analyze the error and convergence order of forward Euler by considering the Taylor series centered at $t_n$:\n",
    "$$\n",
    "    u(t) = u(t_n) + (t - t_n) u'(t_n) + \\frac{u''(t_n)}{2} (t - t_n)^2 + \\mathcal{O}((t-t_n)^3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluating this series at $t_{n+1}$ gives\n",
    "$$\\begin{aligned}\n",
    "    u(t_{n+1}) &= u(t_n) + (t_{n+1} - t_n) u'(t_n) + \\frac{u''(t_n)}{2} (t_{n+1} - t_n)^2 + \\mathcal{O}((t_{n+1}-t_n)^3)\\\\\n",
    "    &=u_n + \\Delta t f(t_n, u_n) + \\frac{u''(t_n)}{2} \\Delta t^2 + \\mathcal{O}(\\Delta t^3)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From the definition of truncation error we can use our Taylor series expression and find the truncation error to be\n",
    "$$\\begin{aligned}\n",
    "    T(t, u; \\Delta t) &= \\frac{1}{\\Delta t} [U_{n+1} - u(t + \\Delta t)] \\\\\n",
    "    &= \\frac{1}{\\Delta t} \\left[ u_n + \\Delta t f(t_n, u_n) - \\left( u_n + \\Delta t f(t_n, u_n) + \\frac{u''(t_n)}{2} \\Delta t^2 + \\mathcal{O}(\\Delta t^3) \\right )\\right ] \\\\\n",
    "    &= \\frac{1}{\\Delta t} \\left[ - \\frac{u''(t_n)}{2} \\Delta t^2 - \\mathcal{O}(\\Delta t^3) \\right ] \\\\\n",
    "    &= - \\frac{u''(t_n)}{2} \\Delta t - \\mathcal{O}(\\Delta t^2)\n",
    "\\end{aligned}$$\n",
    "\n",
    "This implies that forwar Euler is first order accurate and therefore consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error Analysis of Leap-Frog Method\n",
    "\n",
    "To easily analyze this method we will expand the Taylor series from before to another order and evaluate at both the needed positions:\n",
    "$$\n",
    "    u(t) = u(t_n) + (t - t_n) u'(t_n) + (t - t_n)^2 \\frac{u''(t_n)}{2}  + (t - t_n)^3 \\frac{u'''(t_n)}{6} + \\mathcal{O}((t-t_n)^4)\n",
    "$$\n",
    "leading to \n",
    "$$\\begin{aligned}\n",
    "    u(t_{n+1}) &= u_n + \\Delta t f_n + \\Delta t^2 \\frac{u''(t_n)}{2}  + \\Delta t^3 \\frac{u'''(t_n)}{6} + \\mathcal{O}(\\Delta t^4)\\\\\n",
    "    u(t_{n-1}) &= u_n - \\Delta t f_n + \\Delta t^2 \\frac{u''(t_n)}{2}  - \\Delta t^3 \\frac{u'''(t_n)}{6} + \\mathcal{O}(\\Delta t^4)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plugging this into our definition of the truncation error along with the leap-frog method definition leads to\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    T(t, u; \\Delta t) &=\\frac{1}{\\Delta t} \\left [\\left(u_n - \\Delta t f_n + \\Delta t^2 \\frac{u''(t_n)}{2}  - \\Delta t^3 \\frac{u'''(t_n)}{6} + \\mathcal{O}(\\Delta t^4)\\right) + 2\\Delta t f_n - \\left(u_n + \\Delta t f_n + \\Delta t^2 \\frac{u''(t_n)}{2}  + \\Delta t^3 \\frac{u'''(t_n)}{6} + \\mathcal{O}(\\Delta t^4) \\right )\\right ] \\\\\n",
    "    &=\\frac{1}{\\Delta t} \\left [- \\Delta t^3 \\frac{u'''(t_n)}{3} + \\mathcal{O}(\\Delta t^4) \\right ] \\\\\n",
    "    &=- \\Delta t^2 \\frac{u'''(t_n)}{3} + \\mathcal{O}(\\Delta t^3)\n",
    "\\end{aligned}$$\n",
    "Therefore the method is second order accurate and is consistent theoretically.  In practice it's a bit more complicated than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code demo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Compare accuracy between Euler and Leap-Frog\n",
    "f = lambda t, u: -u\n",
    "u_exact = lambda t: numpy.exp(-t)\n",
    "\n",
    "t_f = 10.0\n",
    "num_steps = [2**n for n in xrange(3,12)]\n",
    "delta_t = numpy.empty(len(num_steps))\n",
    "error_euler = numpy.empty(len(num_steps))\n",
    "error_midpoint = numpy.empty(len(num_steps))\n",
    "error_leapfrog = numpy.empty(len(num_steps))\n",
    "\n",
    "for (i, N) in enumerate(num_steps):\n",
    "    t = numpy.linspace(0, t_f, N)\n",
    "    delta_t[i] = t[1] - t[0]\n",
    "    \n",
    "    # Compute Euler solution\n",
    "    u_euler = numpy.empty(t.shape)\n",
    "    u_euler[0] = 1.0\n",
    "    for n in xrange(t.shape[0] - 1):\n",
    "        u_euler[n+1] = u_euler[n] + delta_t[i] * f(t[n], u_euler[n])\n",
    "        \n",
    "    # Compute midpoint\n",
    "    u_midpoint = numpy.empty(t.shape)\n",
    "    u_midpoint[0] = 1.0\n",
    "    for n in xrange(t.shape[0] - 1):\n",
    "        u_midpoint[n+1] = u_midpoint[n] + delta_t[i] * f(t[n] + 0.5 * delta_t[i], \n",
    "                                                         u_midpoint[n] + 0.5 * delta_t[i] * f(t[n], u_midpoint[n]))\n",
    "        \n",
    "    # Compute Leap-Frog\n",
    "    u_leapfrog = numpy.empty(t.shape)\n",
    "    u_leapfrog[0] = 1.0\n",
    "    u_leapfrog[1] = u_euler[1]\n",
    "    for n in xrange(1, t.shape[0] - 1):\n",
    "        u_leapfrog[n+1] = u_leapfrog[n-1] + 2.0 * delta_t[i] * f(t_n, u_leapfrog[n])\n",
    "        \n",
    "    # Compute error for each\n",
    "    error_euler[i] = numpy.abs(u_euler[-1] - u_exact(t_f)) / numpy.abs(u_exact(t_f))\n",
    "    error_midpoint[i] = numpy.abs(u_midpoint[-1] - u_exact(t_f)) / numpy.abs(u_exact(t_f))\n",
    "    error_leapfrog[i] = numpy.abs(u_leapfrog[-1] - u_exact(t_f)) / numpy.abs(u_exact(t_f))\n",
    "    \n",
    "# Plot error vs. delta_t\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 1.5)\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.loglog(delta_t, error_euler, 'ro', label='Forward Euler')\n",
    "axes.loglog(delta_t, error_midpoint, 'bo', label='midpoint')\n",
    "axes.loglog(delta_t, error_leapfrog, 'go', label=\"Leap-Frog\")\n",
    "\n",
    "order_C = lambda delta_x, error, order: numpy.exp(numpy.log(error) - order * numpy.log(delta_x))\n",
    "axes.loglog(delta_t, order_C(delta_t[2], error_euler[2], 1.0) * delta_t**1.0, 'r--', label=\"1st Order\")\n",
    "axes.loglog(delta_t, order_C(delta_t[2], error_midpoint[2], 2.0) * delta_t**2.0, 'b--', label=\"2nd Order\")\n",
    "axes.loglog(delta_t, order_C(delta_t[2], error_leapfrog[2], 2.0) * delta_t**2.0, 'b--')\n",
    "\n",
    "axes.legend(loc=4)\n",
    "axes.set_title(\"Comparison of Errors\")\n",
    "axes.set_xlabel(\"$\\Delta t$\")\n",
    "axes.set_ylabel(\"$|U(t_f) - u(t_f)|$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taylor Series Methods\n",
    "\n",
    "A **Taylor series method** can be derived by direct substitution of the right-hand-side function $f(t, u)$ and it's appropriate derivatives into the Taylor series expansion for $u(t_{n+1})$.  For a $p$th order method we would look at the Taylor series up to that order and replace all the derivatives of $u$ with derivatives of $f$ instead.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the general case we have\n",
    "\\begin{align*}\n",
    "    u(t_{n+1}) = u(t_n) + \\Delta t u'(t_n) + \\frac{\\Delta t^2}{2} u''(t_n) + \\frac{\\Delta t^3}{6} u'''(t_n) + \\cdots + \\frac{\\Delta t^p}{p!} u^{(p)}(t_n)\n",
    "\\end{align*}\n",
    "which contains derivatives of $u$ up to $p$th order.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then replace these derivatives with the appropriate derivative of $f$ which will always be one less than the derivative of $u$ (due to the original ODE)\n",
    "\\[\n",
    "    u^{(p)}(t_n) = f^{(p-1)}(t_n, u(t_n))\n",
    "\\]\n",
    "leading to the method\n",
    "\\[\n",
    "    u(t_{n+1}) = u(t_n) + \\Delta t f(t_n, u(t_n)) + \\frac{\\Delta t^2}{2} f'(t_n, u(t_n)) + \\frac{\\Delta t^3}{6} f''(t_n, u(t_n)) + \\cdots + \\frac{\\Delta t^p}{p!} f^{(p-1)}(t_n, u(t_n)).\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then replace these derivatives with the appropriate derivative of $f$ which will always be one less than the derivative of $u$ (due to the original ODE)\n",
    "\\[\n",
    "    u^{(p)}(t_n) = f^{(p-1)}(t_n, u(t_n))\n",
    "\\]\n",
    "leading to the method\n",
    "\\[\n",
    "    u(t_{n+1}) = u(t_n) + \\Delta t f(t_n, u(t_n)) + \\frac{\\Delta t^2}{2} f'(t_n, u(t_n)) + \\frac{\\Delta t^3}{6} f''(t_n, u(t_n)) + \\cdots + \\frac{\\Delta t^p}{p!} f^{(p-1)}(t_n, u(t_n)).\n",
    "\\]\n",
    "\n",
    "The drawback to these methods is that we have to derive a new one each time we have a new $f$ and we also need $p-1$ derivatives of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2nd Order Taylor Series Method\n",
    "\n",
    "We want terms up to second order so we need to take the derivative of $u' = f(t, u)$ once to find $u'' = f'(t, u)$ and therefore\n",
    "\\begin{align*}\n",
    "    u(t_{n+1}) &= u(t_n) + \\Delta t u'(t_n) + \\frac{\\Delta t^2}{2} u''(t_n) \\\\\n",
    "    &=u(t_n) + \\Delta t f(t_n, u(t_n)) + \\frac{\\Delta t^2}{2} f'(t_n, u(t_n)) ~~~ \\text{or} \\\\\n",
    "    U_{n+1} &= U_n + \\Delta t f(t_n, U_n) + \\frac{\\Delta t^2}{2} f'(t_n, U_n).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Runge-Kutta Methods\n",
    "\n",
    "One way to derive higher-order ODE solvers is by computing intermediate stages.  These are not *multi-step* methods as they still only require information from the current time step but they raise the order of accuracy by adding *stages*.  These types of methods are called **Runge-Kutta** methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example:  Two-stage Runge-Kutta Methods\n",
    "\n",
    "The basic idea behind the simplest of the Runge-Kutta methods is to approximate the solution at $t_n + \\Delta t / 2$ via Euler's method and use this in the function evaluation for the final update.\n",
    "$$\\begin{aligned}\n",
    "    U^* &= U^n + \\frac{1}{2} \\Delta t f(U^n) \\\\\n",
    "    U^{n+1} &= U^n + \\Delta t f(U^*) \\\\\n",
    "    &= U^n + \\Delta t f(U^n + \\frac{1}{2} \\Delta t f(U^n))\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The truncation error can be computed similarly to how we did so before but we do need to figure out how to compute the derivative inside of the function.  Note that due to $f(u(t_n)) = u'(t_n)$ that differentiating this leads to $f'(u(t_n)) u'(t_n) = u''(t_n)$ leading to\n",
    "$$\\begin{aligned}\n",
    "    f\\left(u(t_n) + \\frac{1}{2} \\Delta t f(u(t_n)) \\right ) &= f\\left(u(t_n) +\\frac{1}{2} \\Delta t u'(t_n) \\right ) \\\\\n",
    "    &= f(u(t_n)) + \\frac{1}{2} \\Delta t u'(t_n) f'(u(t_n)) + \\frac{1}{8} \\Delta t^2 (u'(t_n))^2 f''(u(t_n)) + \\mathcal{O}(\\Delta t^3) \\\\\n",
    "    &=u'(t_n) + \\frac{1}{2} \\Delta t u''(t_n) + \\mathcal{O}(\\Delta t^2)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Going back to the truncation error we have\n",
    "$$\\begin{aligned}\n",
    "    T(t, u; \\Delta t) &= \\frac{1}{\\Delta t} \\left[u_n + \\Delta t f\\left(u_n + \\frac{1}{2} \\Delta t f(u_n)\\right) - \\left(u_n + \\Delta t f(t_n, u_n) + \\frac{u''(t_n)}{2} \\Delta t^2 + \\mathcal{O}(\\Delta t^3) \\right ) \\right] \\\\\n",
    "    &=\\frac{1}{\\Delta t} \\left[\\Delta t u'(t_n) + \\frac{1}{2} \\Delta t^2 u''(t_n) + \\mathcal{O}(\\Delta t^3) - \\Delta t u'(t_n) - \\frac{u''(t_n)}{2} \\Delta t^2 + \\mathcal{O}(\\Delta t^3) \\right] \\\\\n",
    "    &= \\mathcal{O}(\\Delta t^2)\n",
    "\\end{aligned}$$\n",
    "\n",
    "so this method is second order accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example:  4-stage Runge-Kutta Method\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    Y_1 &= U_n \\\\\n",
    "    Y_2 &= U_n + \\frac{1}{2} \\Delta t f(Y_1, t_n) \\\\\n",
    "    Y_3 &= U_n + \\frac{1}{2} \\Delta t f(Y_2, t_n + \\Delta t / 2) \\\\\n",
    "    Y_4 &= U_n + \\Delta t f(Y_3, t_n + \\Delta t / 2) \\\\\n",
    "    U_{n+1} &= U_n + \\frac{\\Delta t}{6} \\left [f(Y_1, t_n) + 2 f(Y_2, t_n + \\Delta t / 2) + 2 f(Y_3, t_n + \\Delta t/2) + f(Y_4, t_n + \\Delta t) \\right ]\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code demo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Implement and compare the two-stage and 4-stage Runge-Kutta methods\n",
    "f = lambda t, u: -u\n",
    "\n",
    "t_exact = numpy.linspace(0.0, 10.0, 100)\n",
    "u_exact = numpy.exp(-t_exact)\n",
    "\n",
    "N = 10\n",
    "t = numpy.linspace(0, 10.0, N)\n",
    "delta_t = t[1] - t[0]\n",
    "u_2 = numpy.empty(t.shape)\n",
    "u_4 = numpy.empty(t.shape)\n",
    "u_2[0] = 1.0\n",
    "u_4[0] = 1.0\n",
    "\n",
    "for (n, t_n) in enumerate(t[1:]):\n",
    "    u_2[n+1] = u_2[n] + 0.5 * delta_t * f(t_n, u_2[n])\n",
    "    u_2[n+1] = u_2[n] + delta_t * f(t_n, u_2[n+1])\n",
    "    y_1 = u_4[n]\n",
    "    y_2 = u_4[n] + 0.5 * delta_t * f(t_n + 0.5 * delta_t, y_1)\n",
    "    y_3 = u_4[n] + 0.5 * delta_t * f(t_n + 0.5, y_2)\n",
    "    y_4 = u_4[n] + delta_t * f(t_n + 0.5 * delta_t, y_3)\n",
    "    u_4[n+1] = u_4[n] + delta_t / 6.0 * (f(t_n, y_1) + 2.0 * f(t_n + 0.5 * delta_t, y_2) + 2.0 * f(t_n + 0.5 * delta_t, y_3) + f(t_n + delta_t, y_4))\n",
    "    \n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.plot(t_exact, u_exact, 'k', label=\"True\")\n",
    "axes.plot(t, u_2, 'ro', label=\"2-Stage\")\n",
    "axes.plot(t, u_4, 'bo', label=\"4-Stage\")\n",
    "axes.legend(loc=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Compare accuracy between Euler and Leap-Frog\n",
    "f = lambda t, u: -u\n",
    "u_exact = lambda t: numpy.exp(-t)\n",
    "\n",
    "t_f = 10.0\n",
    "num_steps = [2**n for n in xrange(3,10)]\n",
    "delta_t = numpy.empty(len(num_steps))\n",
    "error_euler = numpy.empty(len(num_steps))\n",
    "error_2 = numpy.empty(len(num_steps))\n",
    "error_4 = numpy.empty(len(num_steps))\n",
    "\n",
    "for (i, N) in enumerate(num_steps):\n",
    "    t = numpy.linspace(0, t_f, N)\n",
    "    delta_t[i] = t[1] - t[0]\n",
    "    \n",
    "    # Compute Euler solution\n",
    "    u_euler = numpy.empty(t.shape)\n",
    "    u_euler[0] = 1.0\n",
    "    for (n, t_n) in enumerate(t[1:]):\n",
    "        u_euler[n+1] = u_euler[n] + delta_t[i] * f(t_n, u_euler[n])\n",
    "        \n",
    "    # Compute 2 and 4-stage\n",
    "    u_2 = numpy.empty(t.shape)\n",
    "    u_4 = numpy.empty(t.shape)\n",
    "    u_2[0] = 1.0\n",
    "    u_4[0] = 1.0\n",
    "    for (n, t_n) in enumerate(t[1:]):\n",
    "        u_2[n+1] = u_2[n] + 0.5 * delta_t[i] * f(t_n, u_2[n])\n",
    "        u_2[n+1] = u_2[n] + delta_t[i] * f(t_n, u_2[n+1])\n",
    "        y_1 = u_4[n]\n",
    "        y_2 = u_4[n] + 0.5 * delta_t[i] * f(t_n + 0.5 * delta_t[i], y_1)\n",
    "        y_3 = u_4[n] + 0.5 * delta_t[i] * f(t_n + 0.5, y_2)\n",
    "        y_4 = u_4[n] + delta_t[i] * f(t_n + 0.5 * delta_t[i], y_3)\n",
    "        u_4[n+1] = u_4[n] + delta_t[i] / 6.0 * (f(t_n, y_1) + 2.0 * f(t_n + 0.5 * delta_t[i], y_2) + 2.0 * f(t_n + 0.5 * delta_t[i], y_3) + f(t_n + delta_t[i], y_4))\n",
    "        \n",
    "    # Compute error for each\n",
    "    error_euler[i] = numpy.abs(u_euler[-1] - u_exact(t_f)) / numpy.abs(u_exact(t_f))\n",
    "    error_2[i] = numpy.abs(u_2[-1] - u_exact(t_f)) / numpy.abs(u_exact(t_f))\n",
    "    error_4[i] = numpy.abs(u_4[-1] - u_exact(t_f)) / numpy.abs(u_exact(t_f))\n",
    "    \n",
    "# Plot error vs. delta_t\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.loglog(delta_t, error_euler, 'ro', label='Forward Euler')\n",
    "axes.loglog(delta_t, error_2, 'bo', label='2-stage')\n",
    "axes.loglog(delta_t, error_4, 'go', label=\"4-stage\")\n",
    "\n",
    "order_C = lambda delta_x, error, order: numpy.exp(numpy.log(error) - order * numpy.log(delta_x))\n",
    "axes.loglog(delta_t, order_C(delta_t[2], error_euler[2], 1.0) * delta_t**1.0, 'r--', label=\"1st Order\")\n",
    "axes.loglog(delta_t, order_C(delta_t[2], error_2[2], 2.0) * delta_t**2.0, 'b--', label=\"2nd Order\")\n",
    "axes.loglog(delta_t, order_C(delta_t[2], error_4[2], 4.0) * delta_t**4.0, 'g--', label=\"4th Order\")\n",
    "\n",
    "axes.legend(loc=4)\n",
    "axes.set_title(\"Comparison of Errors\")\n",
    "axes.set_xlabel(\"$\\Delta t$\")\n",
    "axes.set_ylabel(\"$|U(t_f) - u(t_f)|$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Multi-Step Methods\n",
    "\n",
    "Multi-step methods (as introduced via the leap-frog method) are ODE methods that require multiple time step evaluations to work.  Some of the advanatages of using a multi-step method rather than one-step method included\n",
    "\n",
    " - Taylor series methods require differentiating the given equation which can be cumbersome and difficult to impelent\n",
    " - One-step methods at higher order often require the evaluation of the function $f$ many times\n",
    " \n",
    "Disadvantages\n",
    "\n",
    " - Methods are not self-starting, i.e. they require other methods to find the initial values\n",
    " - The time step $\\Delta t$ in one-step methods can be changed at any time while multi-step methods this is much more complex\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### General Linear Multi-Step Methods\n",
    "\n",
    "All linear multi-step methods can be written as the linear combination of past, present and future solutions:\n",
    "$$\n",
    "    \\sum^r_{j=0} \\alpha_j U_{n+j} = \\Delta t \\sum^r_{j=0} \\beta_j f(U_{n+j}, t_{n+j})\n",
    "$$\n",
    "If $\\beta_r = 0$ then the method is explicit (only requires previous time steps).  Note that the coefficients are not unique as we can multiply both sides by a constant.  In practice a normalization of $\\alpha_r = 1$ is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Adams Methods\n",
    "\n",
    "$$\n",
    "    U_{n+r} = U_{n+r-1} + \\Delta t \\sum^r_{j=0} \\beta_j f(U_{n+j}).\n",
    "$$\n",
    "All these methods have $\\alpha_r = 1$, $\\alpha_{r-1} = -1$ and $\\alpha_j=0$ for $j < r - 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adams-Bashforth Methods\n",
    "The **Adams-Bashforth** methods are explicit solvers that maximize the order of accuracy given a number of steps $r$.  This is accomplished by looking at the Taylor series and picking the coefficients $\\beta_j$ to elliminate as many terms in the Taylor series as possible.\n",
    "$$\\begin{aligned}\n",
    "    \\text{1-step:} & ~ & U_{n+1} &= U_n +\\Delta t f(U_n) \\\\\n",
    "    \\text{2-step:} & ~ & U_{n+2} &= U_{n+1} + \\frac{\\Delta t}{2} (-f(U_n) + 3 f(U_{n+1})) \\\\\n",
    "    \\text{3-step:} & ~ & U_{n+3} &= U_{n+2} + \\frac{\\Delta t}{12} (5 f(U_n) - 16 f(U_{n+1}) + 23 f(U_{n+2})) \\\\\n",
    "    \\text{4-step:} & ~ & U_{n+4} &= U_{n+3} + \\frac{\\Delta t}{24} (-9 f(U_n) + 37 f(U_{n+1}) -59 f(U_{n+2}) + 55 f(U_{n+3}))\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code demo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Use 2-step Adams-Bashforth to compute solution\n",
    "f = lambda t, u: -u\n",
    "\n",
    "t_exact = numpy.linspace(0.0, 10.0, 100)\n",
    "u_exact = numpy.exp(-t_exact)\n",
    "\n",
    "N = 100\n",
    "t = numpy.linspace(0, 10.0, N)\n",
    "delta_t = t[1] - t[0]\n",
    "u_ab2 = numpy.empty(t.shape)\n",
    "\n",
    "# Use RK-2 to start the method\n",
    "u_ab2[0] = 1.0\n",
    "u_ab2[1] = u_ab2[0] + 0.5 * delta_t * f(t[0], u_ab2[0])\n",
    "u_ab2[1] = u_ab2[0] + delta_t * f(t[0], u_ab2[1])\n",
    "for n in xrange(0,len(t)-2):\n",
    "    u_ab2[n+2] = u_ab2[n + 1] + delta_t / 2.0 * (-f(t[n], u_ab2[n]) + 3.0 * f(t[n+1], u_ab2[n+1]))\n",
    "    \n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.plot(t_exact, u_exact, 'k', label=\"True\")\n",
    "axes.plot(t, u_ab2, 'ro', label=\"2-step A-B\")\n",
    "\n",
    "axes.set_title(\"Adams-Bashforth Method\")\n",
    "axes.set_xlabel(\"t\")\n",
    "axes.set_xlabel(\"u(t)\")\n",
    "axes.legend(loc=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adams-Moulton Methods\n",
    "The **Adams-Moulton** methods are the implicit versions of the Adams-Bashforth methods.  Since this gives one additional parameter to use $\\beta_r$ these methods are generally one order of accuracy greater than their counterparts.\n",
    "$$\\begin{aligned}\n",
    "    \\text{1-step:} & ~ & U_{n+1} &= U_n + \\frac{\\Delta t}{2} (f(U_n) + f(U_{n+1})) \\\\\n",
    "    \\text{2-step:} & ~ & U_{n+2} &= U_{n+1} + \\frac{\\Delta t}{12} (-f(U_n) + 8f(U_{n+1}) + 5f(U_{n+2})) \\\\\n",
    "    \\text{3-step:} & ~ & U_{n+3} &= U_{n+2} + \\frac{\\Delta t}{24} (f(U_n) - 5f(U_{n+1}) + 19f(U_{n+2}) + 9f(U_{n+3})) \\\\\n",
    "    \\text{4-step:} & ~ & U_{n+4} &= U_{n+3} + \\frac{\\Delta t}{720}(-19 f(U_n) + 106 f(U_{n+1}) -264 f(U_{n+2}) + 646 f(U_{n+3}) + 251 f(U_{n+4}))\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Truncation Error for Multi-Step Methods\n",
    "\n",
    "We can again find the truncation error in general for linear multi-step methods:\n",
    "$$\\begin{aligned}\n",
    "    T(t, u; \\Delta t) &= \\frac{1}{\\Delta t} \\left [\\sum^r_{j=0} \\alpha_j u_{n+j} - \\Delta t \\sum^r_{j=0} \\beta_j f(u_{n+j}, t_{n+j}) \\right ]\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the general expansion and evalution of the Taylor series about $t_n$ we have\n",
    "$$\\begin{aligned}\n",
    "    u(t_{n+j}) &= u(t_n) + j \\Delta t u'(t_n) + \\frac{1}{2} (j \\Delta t)^2 u''(t_n) + \\mathcal{O}(\\Delta t^3) \\\\\n",
    "    u'(t_{n+j}) &= u'(t_n) + j \\Delta t u''(t_n) + \\frac{1}{2} (j \\Delta t)^2 u'''(t_n) + \\mathcal{O}(\\Delta t^3)\n",
    "\\end{aligned}$$\n",
    "leading to\n",
    "$$\\begin{aligned}\n",
    "    T(t, u; \\Delta t) &= \\frac{1}{\\Delta t}\\left( \\sum^r_{j=0} \\alpha_j\\right) u_{n+j} + \\left(\\sum^r_{j=0} (j\\alpha_j - \\beta_j)\\right) u'(t_n) + \\Delta t \\left(\\sum^r_{j=0} \\left (\\frac{1}{2}j^2 \\alpha_j - j \\beta_j \\right) \\right) u''(t_n) \\\\\n",
    "&~~~~~~~+ \\cdots + \\Delta t^{q - 1} \\left (\\frac{1}{q!} \\left(j^q \\alpha_j - \\frac{1}{(q-1)!} j^{q-1} \\beta_j \\right) \\right) u^{(q)}(t_n) + \\cdots\n",
    "\\end{aligned}$$\n",
    "\n",
    "The method is *consistent* if the first two terms of the expansion vanish, i.e. $\\sum^r_{j=0} \\alpha_j = 0$ and $\\sum^r_{j=0} j \\alpha_j = \\sum^r_{j=0} \\beta_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Predictor-Corrector Methods\n",
    "\n",
    "One way to simplify the Adams-Moulton methods so that implicit evaluations are not needed is by estimating the required implicit function evaluations with an explicit method.  These are often called **predictor-corrector** methods as the explicit method provides a *prediction* of what the solution might be and the not explicit *corrector* step works to make that estimate more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: One-Step Adams-Bashforth-Moulton\n",
    "\n",
    "Use the One-step Adams-Bashforth method to predict the value of $U_{n+1}$ and then use the Adams-Moulton method to correct that value:\n",
    "$$\\begin{aligned}\n",
    "    \\hat{U}_{n+1} &= U_n + \\Delta t f(U_n) \\\\\n",
    "    U_{n+1} &= U_n + \\frac{1}{2} \\Delta t (f(U_n) + f(\\hat{U}_{n+1}) \n",
    "\\end{aligned}$$\n",
    "leading to a second order accurate method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code demo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the Adams-Bashforth-Moulton 2nd order predictor-corrector method\n",
    "f = lambda t, u: -u\n",
    "\n",
    "t_exact = numpy.linspace(0.0, 10.0, 100)\n",
    "u_exact = numpy.exp(-t_exact)\n",
    "\n",
    "N = 50\n",
    "t = numpy.linspace(0, 10.0, N)\n",
    "delta_t = t[1] - t[0]\n",
    "u_abm2 = numpy.empty(t.shape)\n",
    "\n",
    "\n",
    "# Use RK-2 to start the method\n",
    "u_abm2[0] = 1.0\n",
    "u_abm2[1] = u_abm2[0] + 0.5 * delta_t * f(t_n, u_abm2[0])\n",
    "u_abm2[1] = u_abm2[0] + delta_t * f(t_n, u_abm2[1])\n",
    "for n in xrange(len(t) - 1):\n",
    "    u_abm2[n+1] = u_abm2[n] + delta_t * f(t[n], u_abm2[n])\n",
    "    u_abm2[n+1] = u_abm2[n] + 0.5 * delta_t * (f(t[n], u_abm2[n]) + f(t[n+1], u_abm2[n+1]))\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.plot(t_exact, u_exact, 'k', label=\"True\")\n",
    "axes.plot(t, u_abm2, 'ro', label=\"A-B-M\")\n",
    "\n",
    "axes.set_xlabel(\"t\")\n",
    "axes.set_xlabel(\"u(t)\")\n",
    "axes.legend(loc=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence\n",
    "\n",
    "We can think of an ODE method as providing a sequence of approximations $U^N$ where here $N$ is the number of time steps needed to reach the final time of interest $t_f$, in effect we are increasing the resolution of the method.  We then say that a method is convergent if this sequence converges to the true solution at the same time\n",
    "\n",
    "$$\\lim_{N\\rightarrow \\infty} U^N = u(t_f).$$\n",
    "\n",
    "We can also define this in a more familiar way in terms of $\\Delta t$ such that\n",
    "\n",
    "$$N \\Delta t = t_f ~~~~ \\Rightarrow ~~~~ N = \\frac{t_f}{\\Delta t}$$\n",
    "\n",
    "so that our definition of convergence becomes\n",
    "\n",
    "$$\\lim_{\\Delta t \\rightarrow 0} U_{\\Delta t} = u(t_f).$$\n",
    "\n",
    "In general for a method to be convergent it must be\n",
    "\n",
    " - **consistent** which as before meant that the local truncation error $T = \\mathcal{O}(\\Delta t^p)$ where $p > 0$,\n",
    " - **zero-stable** which implies that the sum total of the errors as $\\Delta t \\rightarrow 0$ is bounded and has the same order as $T$ which we know goes to zero as $\\Delta t \\rightarrow 0$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Forward Euler\n",
    "\n",
    "Consider the simple linear problem\n",
    "\n",
    "$$\\frac{\\text{d}u}{\\text{d}t} = - \\lambda u ~~~~ \\text{with}~~~~ u(0) = u_0$$\n",
    "\n",
    "which we know has the solution $u(t) = u_0 e^{\\lambda t}$.  Applying Euler's method to this problem leads to \n",
    "\n",
    "$$\\begin{aligned}\n",
    "U_{n+1} &= U_n + \\Delta t\\lambda U_n \\\\\n",
    "&= (1 + \\Delta t \\lambda) U_n\n",
    "\\end{aligned}$$\n",
    "\n",
    "We also know the local truncation error is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    T^n &= \\left (\\frac{u(t_{n+1}) - u(t_n)}{\\Delta t} \\right ) - \\lambda u(t_n)\\\\\n",
    "    &= \\left (u'(t_n) + \\frac{1}{2} \\Delta t u''(t_n) + \\mathcal{O}(\\Delta t^2) \\right ) - u'(t_n) \\\\\n",
    "    &= \\frac{1}{2} \\Delta t u''(t_n) + \\mathcal{O}(\\Delta t^2)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Noting the original definition of $T_n$ we can rewrite the expression for the local truncation error as\n",
    "\n",
    "$$u(t_{n+1}) = (1 + \\Delta t \\lambda) u(t_n) + \\Delta t T^n$$\n",
    "\n",
    "which in combination with the application of Euler's method leads to an expression for the global error\n",
    "\n",
    "$$E^{n+1} = (1+\\Delta t \\lambda) E^n - \\Delta t T^n$$\n",
    "\n",
    "Expanding this expression out backwards in time to $n=0$ leads to \n",
    "\n",
    "$$E^n = (1 + \\Delta t \\lambda) E^0 - \\Delta t \\sum^n_{i=1} (1 + \\Delta t \\lambda)^{n-i} T^{i - 1}.$$\n",
    "\n",
    "We can now see the importance of the term $(1 + \\Delta t \\lambda)$.  We can bound this term by\n",
    "\n",
    "$e^{\\Delta t \\lambda} \\geq |1 + \\Delta t \\lambda|$\n",
    "\n",
    "which then implies the term in the summation can be bounded by\n",
    "\n",
    "$$|1 + \\Delta t \\lambda|^{n - i} \\leq e^{(n-i) \\Delta t |\\lambda|} \\leq e^{n \\Delta t |\\lambda||} \\leq e^{|\\lambda| t_f}$$\n",
    "\n",
    "Using this expression in the expression for the global error we find\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    E^n &= (1 + \\Delta t \\lambda) E^0 - \\Delta t \\sum^n_{i=1} (1 + \\Delta t \\lambda)^{n-i} T^{i - 1} \\\\\n",
    "    |E^n| &\\leq e^{|\\lambda| \\Delta t} |E^0| - \\Delta t \\sum^n_{i=1} e^{|\\lambda| t_f} |T^{i - 1}| \\\\\n",
    "          &\\leq e^{|\\lambda| t_f} \\left(|E^0| - \\Delta t \\sum^n_{i=1} |T^{i - 1}|\\right) \\\\\n",
    "          &\\leq e^{|\\lambda| t_f} \\left(|E^0| - n \\Delta t \\max_{1 \\leq i \\leq n} |T^{i - 1}|\\right)\n",
    "\\end{aligned}$$\n",
    "In other words the global error is bounded by the original global error and the maximum one-step error made multiplied by the number of time steps taken.  If $N = \\frac{t_f}{\\Delta t}$ as before and taking into account the local truncation error we can simplify this expression further to\n",
    "\n",
    "$$|E^n| \\leq e^{|\\lambda| t_f} \\left[|E^0| + t_f \\left(\\frac{1}{2} \\Delta t |u''| + \\mathcal{O}(\\Delta t^2)\\right ) \\right]$$\n",
    "\n",
    "If we assume that we have used the correct initial condition $u_0$ then $E_0 \\rightarrow 0$ as $\\Delta t \\rightarrow 0$ and we see that the method is truly convergent as \n",
    "\n",
    "$$|E^n| \\leq e^{|\\lambda| t_f} t_f \\left(\\frac{1}{2} \\Delta t |u''| + \\mathcal{O}(\\Delta t^2)\\right ) = \\mathcal{O}(\\Delta t).$$\n",
    "\n",
    "## Absolute Stability\n",
    "Although zero-stability gaurantees stability it is much more difficult to work with in general as the limit $\\Delta t \\rightarrow 0$ can be difficult to compute.  Instead we often consider a finite $\\Delta t$ and examine if the method is stable for this particular choice of $\\Delta t$.  This has the practical upside that it will also tell us what particular $\\Delta t$ will ensure that our method is indeed stable.\n",
    "\n",
    "### Example\n",
    "Consider the problem\n",
    "\n",
    "$$u'(t) = \\lambda (u - \\cos t) - \\sin t ~~~~\\text{with}~~~~ u(0) = 1$$\n",
    "\n",
    "whose exact solution is \n",
    "\n",
    "$$u(t) = \\cos t.$$\n",
    "\n",
    "We can compute an estimate for what $\\Delta t$ we need to use by examining the truncation error \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    T &= \\frac{1}{2} \\Delta t u''(t) + \\mathcal{O}(\\Delta t^2) \\\\\n",
    "      &= -\\frac{1}{2} \\Delta t \\cos t + \\mathcal{O}(\\Delta t^2)\n",
    "\\end{aligned}$$\n",
    "\n",
    "and therfore\n",
    "\n",
    "$$|E^n| \\leq \\Delta t \\max_{0 \\leq t \\leq t_f} |\\cos t| = \\Delta t.$$\n",
    "\n",
    "If we want a solution where $|E^n| < 10^{-3}$ then $\\Delta t \\approx 10^{-3}$.  Turning to the application of Euler's method lets apply this to the case where $\\lambda = -10$ and $\\lambda = -2100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare accuracy between Euler\n",
    "f = lambda t, lam, u: lam * (u - numpy.cos(t)) - numpy.sin(t)\n",
    "u_exact = lambda t: numpy.cos(t)\n",
    "\n",
    "t_f = 2.0\n",
    "# num_steps = [2**n for n in xrange(3, 20)]\n",
    "num_steps = [2**n for n in xrange(15,20)]\n",
    "delta_t = numpy.empty(len(num_steps))\n",
    "error_10 = numpy.empty(len(num_steps))\n",
    "error_2100 = numpy.empty(len(num_steps))\n",
    "\n",
    "for (i, N) in enumerate(num_steps):\n",
    "    t = numpy.linspace(0, t_f, N)\n",
    "    delta_t[i] = t[1] - t[0]\n",
    "    \n",
    "    # Compute Euler solution\n",
    "    U = numpy.empty(t.shape)\n",
    "    U[0] = 1.0\n",
    "    for (n, t_n) in enumerate(t[1:]):\n",
    "        U[n+1] = U[n] + delta_t[i] * f(t_n, -10.0, U[n])\n",
    "    error_10[i] = numpy.abs(U[-1] - u_exact(t_f)) / numpy.abs(u_exact(t_f))\n",
    "    \n",
    "    U = numpy.empty(t.shape)\n",
    "    U[0] = 1.0\n",
    "    for (n, t_n) in enumerate(t[1:]):\n",
    "        U[n+1] = U[n] + delta_t[i] * f(t_n, -2100.0, U[n])\n",
    "    error_2100[i] = numpy.abs(U[-1] - u_exact(t_f)) / numpy.abs(u_exact(t_f))\n",
    "    \n",
    "# Plot error vs. delta_t\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.loglog(delta_t, error_10, 'bo', label='Forward Euler')\n",
    "\n",
    "order_C = lambda delta_x, error, order: numpy.exp(numpy.log(error) - order * numpy.log(delta_x))\n",
    "axes.loglog(delta_t, order_C(delta_t[1], error_10[1], 1.0) * delta_t**1.0, 'r--', label=\"1st Order\")\n",
    "axes.loglog(delta_t, order_C(delta_t[1], error_10[1], 2.0) * delta_t**2.0, 'b--', label=\"2nd Order\")\n",
    "\n",
    "axes.legend(loc=4)\n",
    "axes.set_title(\"Comparison of Errors\")\n",
    "axes.set_xlabel(\"$\\Delta t$\")\n",
    "axes.set_ylabel(\"$|U(t_f) - u(t_f)|$\")\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.loglog(delta_t, error_2100, 'bo', label='Forward Euler')\n",
    "\n",
    "axes.set_title(\"Comparison of Errors\")\n",
    "axes.set_xlabel(\"$\\Delta t$\")\n",
    "axes.set_ylabel(\"$|U(t_f) - u(t_f)|$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what went wrong with $\\lambda = -2100$?  The global error should go as\n",
    "\n",
    "$$E^{n+1} = (1 + \\Delta t \\lambda) E^n - \\Delta t T^n$$\n",
    "\n",
    "If $\\Delta t \\approx 10^{-3}$ then for the case $\\lambda = -10$ the previous global error is multiplied by\n",
    "\n",
    "$$1 + 10^{-3} \\cdot -10 = 0.99$$\n",
    "\n",
    "which means the contribution from $E^n$ will slowly decrease as we take more time steps.  For the other case we have\n",
    "\n",
    "$$1 + 10^{-3} \\cdot -2100 = -1.1$$\n",
    "\n",
    "which means that for this $\\Delta t$ the error made in previous time steps will grow!  For this not to happen we would have to have $\\Delta t < 1 / 2100$ which would lead to convergence again.\n",
    "\n",
    "### Absolute Stability of the Forward Euler Method\n",
    "\n",
    "Consider again the simple test problem $u'(t) = \\lambda u$.  We know from before that applying Euler's method to this problem leads to an update of the form\n",
    "\n",
    "$$U_{n+1} = (1 + \\Delta t \\lambda) U_n.$$\n",
    "\n",
    "As may have been clear from the last example, we know that if\n",
    "\n",
    "$$|1 + \\Delta t \\lambda| \\leq 1$$\n",
    "\n",
    "that the method will be stable, this is called **absolute stability**.  Note that the product of $\\Delta t \\lambda$ is what matters here and often we consider a **region of absolute stability** on the complex plain defined by the equation outlined where now $z = \\Delta t \\lambda$.  This allows the values of $\\lambda$ to be complex which can be an important case to consider, especially for systems of equations where the $\\lambda$s are identified as the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the region of absolute stability for Forward Euler\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "t = numpy.linspace(0.0, 2.0 * numpy.pi, 100)\n",
    "\n",
    "axes.fill(numpy.cos(t) - 1.0, numpy.sin(t), 'b')\n",
    "axes.plot([-3, 3],[0.0, 0.0],'k--')\n",
    "axes.plot([0.0, 0.0],[-3, 3],'k--')\n",
    "axes.set_xlim((-3, 3.0))\n",
    "axes.set_ylim((-3,3))\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.set_title(\"Absolute Stability Region for Forward Euler\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristic Polynomials and Linear Difference Equations\n",
    "\n",
    "As an short aside, say we wanted to solve\n",
    "\n",
    "$$\\sum^r_{j=0} \\alpha_j U_{n+j} = 0$$\n",
    "\n",
    "given initial conditions $U_0, U_1, \\ldots, U_{r-1}$ which has a solution in the general form $U_n = \\xi^n$.  Plugging this into the equation we have\n",
    "\n",
    "$$\\sum^r_{j=0} \\alpha_j \\xi^{n+j} = 0$$\n",
    "\n",
    "which simplifies to\n",
    "\n",
    "$$\\sum^r_{j=0} \\alpha_j \\xi^j = 0 $$\n",
    "\n",
    "by dividing by $\\xi^n$.  If $\\xi$ then is a root of the polynomial\n",
    "\n",
    "$$\\rho(\\xi) = \\sum^r_{j=0} \\alpha_j \\xi^j$$\n",
    "\n",
    "then $\\xi$ solves the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Stability Regions for Linear Multistep Methods\n",
    "\n",
    "Going back to linear multistep methods and applying them in general to our test problem we have\n",
    "\n",
    "$$\\sum^r_{j=0} \\alpha_j U_{n+j} = \\Delta t \\sum^r_{j=0} \\beta_j \\lambda U_{n+j}$$\n",
    "\n",
    "which can be written as \n",
    "\n",
    "$$\\sum^r_{j=0} (\\alpha_j - \\beta_j \\Delta t \\lambda) U_{n+j} = 0$$\n",
    "\n",
    "or using our notation of $z = \\Delta t \\lambda$ we have\n",
    "\n",
    "$$\\sum^r_{j=0} (\\alpha_j - \\beta_j z) U_{n+j} = 0.$$\n",
    "\n",
    "This has a similar form to the linear difference equations considered above!  Letting\n",
    "\n",
    "$$\\rho(\\xi) = \\sum^r_{j=0} \\alpha_j \\xi^j$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\sigma(\\xi) = \\sum^r_{j=0} \\beta_j \\xi^j$$\n",
    "\n",
    "we can write the expression above as\n",
    "\n",
    "$$\\pi(\\xi, z) = \\rho(\\xi) - z \\sigma(\\xi)$$\n",
    "\n",
    "called the **stability polynomial** of the the linear multi-step method.  It turns out that if the roots $\\xi_i$ of this polynomial satisfy\n",
    "\n",
    "$$|\\xi_i| \\leq 1$$\n",
    "\n",
    "then the multi-step method is zero-stable.  We then define the region of absolute stability as the values for $z$ for which this is true.  This approach can also be applied to one-step methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Forward Euler's Method\n",
    "\n",
    "Examining forward Euler's method we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    0 &= U_{n+1} - U_n - \\Delta t \\lambda U_n \\\\\n",
    "    &= U_{n+1} - U_n (1 + \\Delta t \\lambda)\\\\\n",
    "    &= \\xi - 1 (1 + z)\\\\\n",
    "    &=\\pi(\\xi, z)\n",
    "\\end{aligned}$$\n",
    "\n",
    "whose root is $\\xi = 1 + z$ and we have re-derived the stability region we had found before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Stability of the backward Euler Method\n",
    "\n",
    "The backward version of Euler's method is defined as\n",
    "\n",
    "$$U_{n+1} = U_n + \\Delta t f(t_{n+1}, U_{n+1}).$$\n",
    "\n",
    "If we again consider the test problem from before we find that\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    0 &= U_{n+1} (1 - \\Delta t \\lambda) - U_n \\\\\n",
    "    &= \\xi (1 - z) - 1\n",
    "\\end{aligned}$$\n",
    "\n",
    "which has the root $\\xi = \\frac{1}{1 - z}$.  We then have\n",
    "\n",
    "$$\\left|\\frac{1}{1-z}\\right| \\leq 1 \\leftrightarrow |1 - z| \\geq 1$$\n",
    "\n",
    "so in fact the stability region encompasses the entire complex plane except for a circle centered at $(1, 0)$ of radius 1 implying that the backward Euler method is in fact stable for any choice of $\\Delta t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Application to Stiff ODEs\n",
    "\n",
    "Consider again the ODE we examined before\n",
    "\n",
    "$$u'(t) = \\lambda (u - \\cos t) - \\sin t$$\n",
    "\n",
    "except this time with general initial condition $u(t_0) = \\eta$.  What happens to solutions that are slightly different from $\\eta = 1$ or $t_0 = 0$?  The general solution of the ODE is\n",
    "\n",
    "$$u(t) = e^{\\lambda (t - t_0)} (\\eta - \\cos t_0)) + \\cos t$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot \"hairy\" solutions to the ODE\n",
    "u = lambda t_0, eta, lam, t: numpy.exp(lam * (t - t_0)) * (eta - numpy.cos(t_0)) + numpy.cos(t)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for lam in [-1, -10]:\n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_subplot(1, 1, 1)\n",
    "    for eta in numpy.linspace(-1, 1, 10):\n",
    "        for t_0 in numpy.linspace(0.0, 9.0, 10):\n",
    "            t = numpy.linspace(t_0,10.0,100)\n",
    "            axes.plot(t, u(t_0, eta, lam, t),'b')\n",
    "    t = numpy.linspace(0.0,10.0,100)\n",
    "    axes.plot(t, numpy.cos(t), 'r', linewidth=5)\n",
    "        \n",
    "    axes.set_title(\"Perturbed Solutions $\\lambda = %s$\" % lam)\n",
    "    axes.set_xlabel('$t$')\n",
    "    axes.set_ylabel('$u(t)$')\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot \"inverse hairy\" solutions to the ODE\n",
    "u = lambda t_0, eta, lam, t: numpy.exp(lam * (t - t_0)) * (eta - numpy.cos(t_0)) + numpy.cos(t)\n",
    "\n",
    "fig = plt.figure()\n",
    "num_steps = 10\n",
    "error = numpy.ones(num_steps) * 1.0\n",
    "t_hat = numpy.linspace(0.0, 10.0, num_steps + 1)\n",
    "t_whole = numpy.linspace(0.0, 10.0, 1000)\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "eta = 1.0\n",
    "lam = 0.1\n",
    "\n",
    "for n in xrange(1,num_steps):\n",
    "    t = numpy.linspace(t_hat[n-1], t_hat[n], 100)\n",
    "    U = u(t_hat[n-1], eta, lam, t)\n",
    "    axes.plot(t, U, 'b')\n",
    "    axes.plot(t_whole, u(t_hat[n-1], eta, lam, t_whole),'b--')\n",
    "    axes.plot([t[-1], t[-1]], (U[-1], U[-1] + -1.0**n * error[n]), 'r')\n",
    "    eta = U[-1] + -1.0**n * error[n]\n",
    "\n",
    "t = numpy.linspace(0.0, 10.0, 100)\n",
    "axes.plot(t, numpy.cos(t), 'g')\n",
    "\n",
    "axes.set_title(\"Perturbed Solutions $\\lambda = %s$\" % lam)\n",
    "axes.set_xlabel('$t$')\n",
    "axes.set_ylabel('$u(t)$')\n",
    "axes.set_ylim((-10,10))\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Chemical systems\n",
    "\n",
    "Consider the transition of a chemical $A$ to a chemical $C$ through the process\n",
    "\n",
    "$$A \\overset{K_1}{\\rightarrow} B \\overset{K_2}{\\rightarrow} C.$$\n",
    "\n",
    "If we let\n",
    "\n",
    "$$\\vec{u} = \\begin{bmatrix} [A] \\\\ [B] \\\\ [C] \\end{bmatrix}$$\n",
    "\n",
    "then we can model this simple chemical reaction with the system of ODEs\n",
    "\n",
    "$$\\frac{\\text{d} \\vec{u}}{\\text{d} t} = \n",
    "\\begin{bmatrix}\n",
    "    -K_1 & 0 & 0 \\\\\n",
    "    K_1 & -K_2 & 0 \\\\\n",
    "    0 & K_2 & 0\n",
    "\\end{bmatrix} \\vec{u}$$\n",
    "\n",
    "The solution of this system is of the form\n",
    "\n",
    "$$u_j(t) = c_{j1} e^{-K_1 t} + c_{j2}e^{-K_2 t} + c_{j3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solve the chemical systems example\n",
    "# Problem parameters\n",
    "# K_1 = 3\n",
    "# K_2 = 1\n",
    "K_1 = 30.0\n",
    "K_2 = 1.0\n",
    "\n",
    "A = numpy.array([[-K_1, 0, 0], [K_1, -K_2, 0], [0, K_2, 0]])\n",
    "f = lambda u: numpy.dot(A, u)\n",
    "\n",
    "t = numpy.linspace(0.0, 8.0, 128)\n",
    "delta_t = t[1] - t[0]\n",
    "\n",
    "U = numpy.empty((t.shape[0], 3))\n",
    "U[0, :] = [2.5, 5.0, 2.0]\n",
    "\n",
    "for n in xrange(t.shape[0] - 1):\n",
    "    U[n+1, :] = U[n, :] + delta_t * f(U[n, :])\n",
    "    \n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.plot(t, U)\n",
    "\n",
    "axes.set_title(\"Chemical System\")\n",
    "axes.set_xlabel(\"$t$\")\n",
    "axes.set_title(\"$[A], [B], [C]$\")\n",
    "axes.set_ylim((0.0, 10.))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is stiffness?\n",
    "\n",
    "In general a **stiff** ODE is one where $u'(t) \\ll f'(t, u)$.  For systems of ODEs the **stiffness ratio**\n",
    "\n",
    "$$\\frac{\\max_p |\\lambda_p|}{\\min_p |\\lambda_p|}$$\n",
    "\n",
    "can be used to characterize the stiffness of the system.  In our last example this ratio was $K_1 / K_2$ if $K_1 > K_2$.  As we increased this ratio we observed that the numerical method became unstable only a reduction in $\\Delta t$ lead to stable solution again.  For explicit time step methods this is problematic as the reduction of the time step for only one of the species leads to very expensive evaluations.  For example, forward Euler has the stability criteria\n",
    "\n",
    "$$|1 + \\Delta t \\lambda| < 1$$\n",
    "\n",
    "where $\\lambda$ will have to be the maximum eigenvalue of the system.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the region of absolute stability for Forward Euler\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "t = numpy.linspace(0.0, 2.0 * numpy.pi, 100)\n",
    "\n",
    "K_1 = 30.0\n",
    "K_2 = 1.0\n",
    "delta_t = 1.0\n",
    "eigenvalues = [-K_1, -K_2]\n",
    "\n",
    "axes.fill(numpy.cos(t) - 1.0, numpy.sin(t), color=(255.0/255.0,145.0/255.0,0/255.0,1.0))\n",
    "for lam in eigenvalues:\n",
    "    print lam * delta_t\n",
    "    axes.plot(lam * delta_t, 0.0, 'ko')\n",
    "axes.plot([-3, 3],[0.0, 0.0],'k--')\n",
    "axes.plot([0.0, 0.0],[-3, 3],'k--')\n",
    "# axes.set_xlim((-3, 1))\n",
    "axes.set_ylim((-2,2))\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.set_title(\"Absolute Stability Region for Forward Euler\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-Stability\n",
    "What if we could expand the absolute stability region to encompass more of the left-half plane or even better, all of it.  A method that has this property is called **A-stable**.  We have already seen one example of this with backward Euler which as a stability region of\n",
    "\n",
    "$$|1 - z| \\geq 1$$\n",
    "\n",
    "which covers the full left-half plane.  It turns out that for linear multi-step methods a theorem by Dahlquist proves that there are no LMMs that satisfies the A-stability criterion beyond second order (trapezoidal rule).  There are higher-order Runge-Kutta methods do however.\n",
    "\n",
    "Perhaps this is too restrictive though.  Often large eigenvalues for systems (for instance coming from a PDE discretization for the heat equation) lie completely on the real line.  If the stability region can encompass as much of the real line as possible while leaving out the rest of the left-half plane we can possibly get a more efficient method.  There are a number of methods that can be constructed that have this property but are higher-order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the region of absolute stability for Forward Euler\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "t = numpy.linspace(0.0, 2.0 * numpy.pi, 100)\n",
    "\n",
    "K_1 = 3.0\n",
    "K_2 = 1.0\n",
    "delta_t = 1.0\n",
    "eigenvalues = [-K_1, -K_2]\n",
    "\n",
    "axes.set_axis_bgcolor((255.0/255.0,145.0/255.0,0/255.0,1.0))\n",
    "axes.fill(numpy.cos(t) + 1.0, numpy.sin(t), 'w')\n",
    "for lam in eigenvalues:\n",
    "    print lam * delta_t\n",
    "    axes.plot(lam * delta_t, 0.0, 'ko')\n",
    "axes.plot([-3, 3],[0.0, 0.0],'k--')\n",
    "axes.plot([0.0, 0.0],[-3, 3],'k--')\n",
    "# axes.set_xlim((-3, 1))\n",
    "axes.set_ylim((-2,2))\n",
    "axes.set_aspect('equal')\n",
    "\n",
    "axes.set_title(\"Absolute Stability Region for Backward Euler\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-Stability\n",
    "\n",
    "It turns out not all A-stable methods are alike.  Consider the backward Euler method and the trapezoidal method defined by\n",
    "\n",
    "$$\\frac{U_{n+1} - U_n}{\\Delta t} = \\frac{1}{2}(f(U_n) + f(U_{n+1})$$\n",
    "\n",
    "whose stability polynomial is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    0 &= U_{n+1} - U_n - \\Delta t \\frac{1}{2} (\\lambda U_n + \\lambda U_{n+1}) \\\\\n",
    "      &= U_{n+1}\\left(1 - \\frac{1}{2} \\Delta t \\lambda \\right ) - U_n \\left(1 + \\frac{1}{2}\\Delta t \\lambda \\right) \\\\\n",
    "      &= \\left(\\xi - \\frac{1 + \\frac{1}{2}z}{1 - \\frac{1}{2} z}\\right) \\left(1 - \\frac{1}{2} z \\right )\\\\\n",
    "\\end{aligned}$$\n",
    "which shows that it is A-stable.  Lets apply both these methods to a problem we have seen before and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare accuracy between Euler\n",
    "f = lambda t, lam, u: lam * (u - numpy.cos(t)) - numpy.sin(t)\n",
    "u_exact = lambda t_0, eta, lam, t: numpy.exp(lam * (t - t_0)) * (eta - numpy.cos(t_0)) + numpy.cos(t)\n",
    "\n",
    "t_0 = 0.0\n",
    "eta = 1.5\n",
    "lam = -1e6\n",
    "\n",
    "num_steps = [10, 20, 40, 50]\n",
    "\n",
    "delta_t = numpy.empty(len(num_steps))\n",
    "error_euler = numpy.empty(len(num_steps))\n",
    "error_trap = numpy.empty(len(num_steps))\n",
    "\n",
    "for (i, N) in enumerate(num_steps):\n",
    "    t = numpy.linspace(0, t_f, N)\n",
    "    delta_t[i] = t[1] - t[0]\n",
    "    u = u_exact(t_0, eta, lam, t_f)\n",
    "    \n",
    "    # Compute Euler solution\n",
    "    U_euler = numpy.empty(t.shape)\n",
    "    U_euler[0] = eta\n",
    "    for (n, t_n) in enumerate(t[1:]):\n",
    "        U_euler[n+1] = (U_euler[n] - lam * delta_t[i] * numpy.cos(t_n) - delta_t[i] * numpy.sin(t_n)) / (1.0 - lam * delta_t[i])\n",
    "    error_euler[i] = numpy.abs(U_euler[-1] - u) / numpy.abs(u)\n",
    "    \n",
    "    # Compute using trapezoidal\n",
    "    U_trap = numpy.empty(t.shape)\n",
    "    U_trap[0] = eta\n",
    "    for (n, t_n) in enumerate(t[1:]):\n",
    "        U_trap[n+1] = (U_trap[n] + delta_t[i] * 0.5 * f(t_n, lam, U_trap[n]) - 0.5 * lam * delta_t[i] * numpy.cos(t_n) - 0.5 * delta_t[i] * numpy.sin(t_n)) / (1.0 - 0.5 * lam * delta_t[i])\n",
    "    error_trap[i] = numpy.abs(U_trap[-1] - u)\n",
    "    \n",
    "# Plot error vs. delta_t\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(t, U_euler, 'ro-')\n",
    "axes.plot(t, u_exact(t_0, eta, lam, t),'k')\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.loglog(delta_t, error_euler, 'bo')\n",
    "\n",
    "order_C = lambda delta_x, error, order: numpy.exp(numpy.log(error) - order * numpy.log(delta_x))\n",
    "axes.loglog(delta_t, order_C(delta_t[1], error_euler[1], 1.0) * delta_t**1.0, 'r--', label=\"1st Order\")\n",
    "axes.loglog(delta_t, order_C(delta_t[1], error_euler[1], 2.0) * delta_t**2.0, 'b--', label=\"2nd Order\")\n",
    "\n",
    "axes.legend(loc=4)\n",
    "axes.set_title(\"Comparison of Errors for Backwards Euler\")\n",
    "axes.set_xlabel(\"$\\Delta t$\")\n",
    "axes.set_ylabel(\"$|U(t_f) - u(t_f)|$\")\n",
    "\n",
    "# Plots for trapezoid\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(t, U_trap, 'ro-')\n",
    "axes.plot(t, u_exact(t_0, eta, lam, t),'k')\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.loglog(delta_t, error_trap, 'bo', label='Forward Euler')\n",
    "\n",
    "order_C = lambda delta_x, error, order: numpy.exp(numpy.log(error) - order * numpy.log(delta_x))\n",
    "axes.loglog(delta_t, order_C(delta_t[1], error_trap[1], 1.0) * delta_t**1.0, 'r--', label=\"1st Order\")\n",
    "axes.loglog(delta_t, order_C(delta_t[1], error_trap[1], 2.0) * delta_t**2.0, 'b--', label=\"2nd Order\")\n",
    "\n",
    "axes.legend(loc=4)\n",
    "axes.set_title(\"Comparison of Errors for Trapezoidal Rule\")\n",
    "axes.set_xlabel(\"$\\Delta t$\")\n",
    "axes.set_ylabel(\"$|U(t_f) - u(t_f)|$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that if we look at a one-step method and define the following ratio\n",
    "\n",
    "$$U_{n+1} = R(z) U_n$$\n",
    "\n",
    "we can define another form of stability, called **L-stable**, where we require that the method is A-stable and that\n",
    "\n",
    "$$\\lim_{z \\rightarrow \\infty} |R(z)| = 0.$$\n",
    "\n",
    "Turns out that backwards Euler is L-stable while trapezoidal rule is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Differencing Formulas\n",
    "\n",
    "A class of LMM methods that are useful for stiff ODE problems are the backward difference formula (BDF) methods which have the form\n",
    "\n",
    "$$\\alpha_0 U_n + \\alpha_1 U_{n+1} + \\cdots + \\alpha_r U_{n+r} = \\Delta \\beta_r f(U_{n+r})$$\n",
    "\n",
    "These methods can be derived directly from backwards finite differences from the point $U_{n+r}$ and the rest of the points back in time.  One can then derive r-step methods that are rth-order accurate this way.  Some of the methods are \n",
    "\n",
    "$$\\begin{aligned}\n",
    "    r = 1:& & U_{n+1} - U_n = \\Delta t f(U_{n+1}) \\\\\n",
    "    r = 2:& &3 U_{n+2} - 4 U_{n+1} + U_n = 2 \\Delta t f(U_{n+1}) \\\\\n",
    "    r = 3:& &11U_{n+3} - 18U_{n+2} + 9U_{n+1} - 2 U_n = 6 \\Delta t f(U_{n+3}) \\\\\n",
    "    r = 4:& &25 U_{n+4} - 48 U_{n+3} +36 U_{n+2} -16 U_{n+1} +3 U_n = 12 \\Delta t f(U_{n+4})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Stability Regions\n",
    "\n",
    "If we think of the roots of the stability polynomial $\\xi_j$ as complex numbers and write them in exponential form \n",
    "\n",
    "$$\\xi_j = |\\xi_j| e^{i \\theta}.$$ \n",
    "\n",
    "Here $|\\xi_j|$ is the modulus (or magnitude) or the complex number and is defined as $|\\xi_j| = x^2 + y^2$ where $\\xi_j = x + i j$.  If the $\\xi_j$s are on the boundary of the absolute stability region then we know that $|\\xi_j| = 1$.  Using this in conjunction with the stability polynomial then leads to \n",
    "\n",
    "$$\\rho(e^{i\\theta}) - z \\sigma(e^{i\\theta}) = 0$$\n",
    "\n",
    "which solving for $z$ leads to\n",
    "\n",
    "$$z(\\theta) = \\frac{\\rho(e^{i\\theta})}{\\sigma(e^{i\\theta})}.$$\n",
    "\n",
    "This does not necessarily ensure that given a $\\theta$ that $z(\\theta)$ will lie on the absolute stability region's boundary.  This can occur when $\\xi_j = 1$ but to the left and right of the curve $\\xi_j > 1$ and so therefore does not mark the boundary of the region.  To determine whether a particular region outlined by this curve is inside or outside of the stability region we can evaluate all the roots of $\\pi(\\xi, z)$ at some $z$ inside of the region in question and see if then $\\forall j, \\xi_j < 1$.\n",
    "\n",
    "For one-step methods this becomes easier, if we look at the ratio $R(z)$ we defined earlier for pth order Taylor series method applied to $u'(t) = \\lambda u$ we get\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    U_{n+1} &= U_n + \\Delta t \\lambda U_n + \\frac{1}{2}\\Delta t^2 \\lambda^2 U_n + \\cdots + \\frac{1}{p!}\\Delta t^p \\lambda^p U_n \\\\\n",
    "    &=\\left(1 + z + \\frac{1}{2} z^2 + \\frac{1}{6} z^3 + \\cdots +\\frac{1}{p!}z^p\\right) U_n \\Rightarrow \\\\\n",
    "    R(z) &= 1 + z + \\frac{1}{2} z^2 + \\frac{1}{6} z^3 + \\cdots +\\frac{1}{p!}z^p.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Setting $R(z) = e^{i\\theta}$ could lead to a way for solving for the boundary but (where $|R(z)| = 1$) but this is very difficult to do in general.  Instead if we plot the contours of $R(z)$ in the complex plain we can pick out the $R(z)=1$ contour and plot that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = numpy.linspace(0.0, 2.0 * numpy.pi, 100)\n",
    "\n",
    "# ==================================\n",
    "#  Forward euler\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 2.0)\n",
    "fig.set_figheight(fig.get_figheight() * 2.0)\n",
    "axes = fig.add_subplot(2, 2, 1)\n",
    "x = numpy.linspace(-2, 2, 100)\n",
    "y = numpy.linspace(-2, 2, 100)\n",
    "X, Y = numpy.meshgrid(x, y)\n",
    "Z = X + 1j * Y\n",
    "\n",
    "# Evaluate which regions are actually in stability region\n",
    "print \"Forward Euler's method - Stability region tests\"\n",
    "z = -1 + 1j * 0\n",
    "print \"  Inside of circle: \", numpy.abs(1.0 + z)\n",
    "z = -3 + 1j * 0\n",
    "print \"  Outside of circle: \", numpy.abs(1.0 + z)\n",
    "\n",
    "axes.contour(X, Y, numpy.abs(1.0 + Z), levels=[1.0])\n",
    "axes.plot(x, numpy.zeros(x.shape),'k')\n",
    "axes.plot(numpy.zeros(y.shape), y,'k')\n",
    "axes.set_aspect('equal')\n",
    "axes.set_title(\"Forward Euler\")\n",
    "\n",
    "# ==================================\n",
    "#  Backwards Euler\n",
    "axes = fig.add_subplot(2, 2, 2)\n",
    "x = numpy.linspace(-2, 2, 100)\n",
    "y = numpy.linspace(-2, 2, 100)\n",
    "X, Y = numpy.meshgrid(x, y)\n",
    "Z = X + 1j * Y\n",
    "\n",
    "# Evaluate which regions are actually in stability region\n",
    "print \"Backward Euler's method - Stability region tests\"\n",
    "z = 1 + 1j * 0\n",
    "print \"  Inside of circle: \", numpy.abs(1.0 + z)\n",
    "z = -1 + 1j * 0\n",
    "print \"  Outside of circle: \", numpy.abs(1.0 + z)\n",
    "\n",
    "axes.contour(X, Y, numpy.abs(1.0 / (1.0 - Z)), levels=[1.0])\n",
    "axes.plot(x, numpy.zeros(x.shape),'k')\n",
    "axes.plot(numpy.zeros(y.shape), y,'k')\n",
    "axes.set_aspect('equal')\n",
    "axes.set_title(\"Backwards Euler\")\n",
    "\n",
    "# ==================================\n",
    "#  Taylor series method of order 4\n",
    "axes = fig.add_subplot(2, 2, 3)\n",
    "x = numpy.linspace(-5, 5, 100)\n",
    "y = numpy.linspace(-5, 5, 100)\n",
    "X, Y = numpy.meshgrid(x, y)\n",
    "Z = X + 1j * Y\n",
    "\n",
    "# Evaluate which regions are actually in stability region\n",
    "print \"Taylor series method of order 4 - Stability region tests\"\n",
    "z = -1 + 1j * 0\n",
    "print \"  Inside of strange region: \", numpy.abs(1.0 + z)\n",
    "z = -4 + 1j * 0\n",
    "print \"  Outside of strange region: \", numpy.abs(1.0 + z)\n",
    "\n",
    "axes.contour(X, Y, numpy.abs(1 + Z + 0.5 * Z**2 + 1.0/6.0 * Z**3 + 1.0 / 24.0 * Z**4), levels=[1.0])\n",
    "axes.plot(x, numpy.zeros(x.shape),'k')\n",
    "axes.plot(numpy.zeros(y.shape), y,'k')\n",
    "axes.set_aspect('equal')\n",
    "axes.set_title(\"4th Order Taylor Series\")\n",
    "\n",
    "# ==================================\n",
    "# 2-step Adams-Bashforth\n",
    "theta = numpy.linspace(0.0, 2.0 * numpy.pi, 1000)\n",
    "xi = numpy.exp(1j * theta)\n",
    "\n",
    "rho_2AB = lambda xi: (xi - 1.0) * xi\n",
    "sigma_2AB = lambda xi: (3.0 * xi - 1.0) / 2.0\n",
    "z_2AB = rho_2AB(xi) / sigma_2AB(xi)\n",
    "z = rho_2AB(xi) / sigma_2AB(xi)\n",
    "\n",
    "axes = fig.add_subplot(2, 2, 4)\n",
    "axes.plot(z_2AB.real, z_2AB.imag)\n",
    "axes.plot(x, numpy.zeros(x.shape),'k')\n",
    "axes.plot(numpy.zeros(y.shape), y,'k')\n",
    "axes.set_title(\"2-step Adams-Bashforth\")\n",
    "axes.set_aspect('equal')\n",
    "axes.set_xlim([-2, 3])\n",
    "axes.set_ylim([-2, 2])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
