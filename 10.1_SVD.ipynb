{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "Motivation: The image of the unit sphere under any $m \\times n$ matrix is a hyperellipse.\n",
    "\n",
    "Definition: Any real $m \\times n$ matrix can be factored as\n",
    "$$A = U\\Sigma V^{T}$$\n",
    "\n",
    "where,\n",
    "* $U$ is an $m \\times m$ orthogonal matrix whose columns are the eigenvectors of $AA^{T}$. also known as left singular\n",
    "* $V$ is an $n \\times n$ orthogonal matrix whose columns are the eigenvectors of $A^{T}A$. also known as right singular\n",
    "* $\\Sigma$ is an $m \\times n$ diagonal matrix, with elements $\\sigma_{1}, \\sigma_{2}, \\sigma_{3}, ... \\sigma_{r}$ where $r = rank(A)$ corresponding to the square roots of the eigenvalues of $A^{T}A$. They are called the singular values of $A$ and are non negative arranged in descending order. ($\\sigma_{1} \\geq \\sigma_{2} \\geq \\sigma_{3} \\geq ... \\sigma_{r} \\geq 0$)\n",
    "\n",
    "\n",
    "method is frequently used in dimensionality reduction. That is, selecting first $k$ $(k < r)$ eigenvalues in $\\Sigma$ and reconstructing the matrix $A$ to obtain $A'$, by selecting first $k$ eigenvalues we ensure that $A'$ retains a high amount of the variance of $A$.\n",
    "\n",
    "\n",
    "### Full SVD example\n",
    "\n",
    "consider, $$A = \\begin{bmatrix} 2 & 0 & 3 \\\\ 5 & 7 & 1 \\\\ 0 & 6 & 2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [2,0,3],\n",
    "    [5,7,1],\n",
    "    [0,6,2]\n",
    "])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AAT = np.matmul(A, A.T)\n",
    "ATA = np.matmul(A.T, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U = eigenvector(AA^{T})$\n",
    "\n",
    "\n",
    "$$U = eig(AA^{T}) = eig(\\begin{bmatrix} 2 & 0 & 3 \\\\ 5 & 7 & 1 \\\\ 0 & 6 & 2 \\end{bmatrix} \\begin{bmatrix} 2 & 5 & 0 \\\\ 0 & 7 & 6 \\\\ 3 & 1 & 2 \\end{bmatrix}) = eig(\\begin{bmatrix} 13 & 13 & 6 \\\\ 13 & 75 & 44 \\\\ 6 & 44 & 40 \\end{bmatrix})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U = np.linalg.eig(AAT)[1]\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V^{T} = eigenvector(A^{T}A)^T $\n",
    "\n",
    "$$V = eig(A^{T}A) = eig(\\begin{bmatrix} 2 & 5 & 0 \\\\ 0 & 7 & 6 \\\\ 3 & 1 & 2 \\end{bmatrix} \\begin{bmatrix} 2 & 0 & 3 \\\\ 5 & 7 & 1 \\\\ 0 & 6 & 2 \\end{bmatrix} = eig(\\begin{bmatrix} 29 & 35 & 11 \\\\ 35 & 85 & 19 \\\\ 11 & 19 & 14 \\end{bmatrix})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.linalg.eig(ATA)[1]\n",
    "V_transpose = V.transpose()\n",
    "print(V_transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Sigma$ = diagonal matrix with eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_eigvals = np.linalg.eig(AAT)[0]\n",
    "V_eigvals = np.linalg.eig(ATA)[0]\n",
    "print(U_eigvals, V_eigvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = np.sqrt(np.diag(abs(V_eigvals)))\n",
    "print(diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction of matrix $A$\n",
    "\n",
    "$$A = U\\Sigma V^{T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dash = np.matmul(U, diag)\n",
    "A_dash = np.matmul(A_dash, V_transpose)\n",
    "print(A_dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, diag, vt = np.linalg.svd(A, full_matrices=True)\n",
    "print(u,'\\n \\n',diag,'\\n \\n', vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reduced SVD__: Since computing SVD for all of eigenvectors is cumbersome, one can compute only till $k$ values that will be considered for the matrix reduction. Now the matrix dimensions becomes, $U_{mk}, \\Sigma_{kk}, V_{kn}^{T}$.  usually $k = min(m, n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD vs Eigenvalue decomposition\n",
    "If the columns of a matrix $X$ contain linearly independent eigenvectors of $A$, the eigenvalue decomposition of $A$ is,\n",
    "$$A = X \\Lambda X^{-1}$$\n",
    "\n",
    "so whats the difference?\n",
    "* SVD used two different bases (sets of left and right singular values), whereas the eigenvalue decompositon uses just one(the eigenvectors).\n",
    "* SVD uses orthonormal bases, whereas eigenvalue decomposition uses basis that generally is not orthogonal.\n",
    "* Not all matrices(even square ones) have an eigenvalue decompostion but all matrices(even rectangular ones) have SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existence and Uniqueness\n",
    "\n",
    "__Theorem 1.__ Every matrix $A \\in \\mathbb{C}^{m \\times n}$ has a singular value decompostion. Furthermore, the singular values $\\{\\sigma_{j}\\}$ are uniquely determined, and if $A$ is square and the $\\sigma_{j}$ are distinct, the left and right singular vectors $\\{u_{j}\\}$ and $\\{v_{j}\\}$ are uniquely determined up to complex signs (i.e., complex scalar factors of absolute value 1).\n",
    "\n",
    "\n",
    "\n",
    "### Matrix Properties via the SVD\n",
    "\n",
    "__Theorem 2.__ _The rank of $A$ is $r$, the number of nonzero singular values._\n",
    "\n",
    "Proof, The rank of a diagonal matrix is equal to the number of its nonzero entries, and in the decompostion $A=U\\Sigma V^{T}$, $U$ and $V$ are of full rank. Therefore $rank(A) = rank(\\Sigma) = r$\n",
    "\n",
    "__Theorem 3.__ $range(A) = \\langle u_1, ... , u_r \\rangle$ _and_ $null(a) = \\langle v_{r+1}, ... , v_n\\rangle$\n",
    "\n",
    "Proof, This is a consequence of the fact that $range(\\Sigma) = \\langle e_1, ..., e_r \\rangle \\subseteq \\mathbb{C}^{m}$ and $null(\\Sigma) = \\langle e_{r+1}, ... , e_n \\rangle \\subseteq \\mathbb{C}^{n}$\n",
    "\n",
    "__Theorem 4.__ $\\| A \\|_2 = \\sigma_1$ and $\\|A\\|_F = \\sqrt{\\sigma_{1}^{2}+\\sigma_{2}^{2}+...+\\sigma_{r}^{2}}$\n",
    "\n",
    "Proof, The first result was already established in the proof of Theorem 2: Since $A = U\\Sigma V^{T}$ with unitary $U$ and $V$, $\\| A \\|_2 = \\| \\Sigma \\|_2 = max\\{|\\sigma_j|\\} = \\sigma_1.$ The frobenius norm is invariant under unitary multiplication, so $\\|A\\|_F = \\|\\Sigma\\|_F$\n",
    "\n",
    "__Theorem 5.__ The nonzero singular values of A are the square roots of the nonzero eigenvalues of $A^{T}A$ or $AA^{T}$. (These matrices have the same nonzero eigenvalues.)\n",
    "\n",
    "Proof, $$A^{T}A = (U\\Sigma V^{T})^{T}(U \\Sigma V^{T}) = V \\Sigma^{T} U^{T} U \\Sigma V^{T} = V(\\Sigma^{T}\\Sigma) V^{T}$$\n",
    "We see that $A^{T}A$ is similar to $\\Sigma*\\Sigma$ and hence has the same $n$ eigenvalues. The eigenvalues of the diagonal matrix $\\Sigma*\\Sigma$ are $\\sigma_{1}^{2}, \\sigma_{2}^{2}, ..., \\sigma_{p}^{2}$ with $n-p$ additional zero eigenvalues if $n>p$. A similar calculation applies to the m eigenvalues of $A*A^{T}$\n",
    "\n",
    "__Theorem 6.__ if $A = A^{T}$, then the singular values of $A$ are the absolute values of the eigenvalues of $A$.\n",
    "\n",
    "\n",
    "Proof, a hermitian matrix has a complete set of orthogonal eigenvectors, and all of the eigenvalues are real. An equivalent statement is that $A=X\\Lambda X^{-1}$ holds with $X$ equal to some unitary matrix $Q$ and $\\Lambda$ a real diagonal matrix. But then we can write $$ A = Q\\Lambda Q^{T} = Q|\\Lambda|sign(\\Lambda)Q^{T}$$ where $|\\Lambda|$ and $sign(\\Lambda)$ denote the diagonal matrices whose entries are the numbers $|\\lambda_j|$ and $sign(\\lambda_j)$ respectively. Since $sign(\\Lambda)Q^{T}$ is unitary whenever $Q$ is unitary. the equation is an SVD of $A$.\n",
    "\n",
    "__Theorem 7.__ _for $A \\in \\mathbb{C}^{m \\times n}$, $|det(A)| = \\Pi_{i=1}^{m} \\sigma_{i}$_\n",
    "\n",
    "Proof, The determinant of a product of square matrices is the product of the determinants of the factors. Furthermore, the determinants of a unitary matrix is always 1 in absolute value; this follows from the formula $U^{T}U = I$ and the property $det(U^T) = (det(U))^T$. Therefore, $$|det(A)| = |det(U\\Sigma V^T)| = |det(U)||det(\\Sigma)||det(V^T)| = |det(\\Sigma)| = \\Pi_{i=1}^{m}\\sigma_i$$\n",
    "\n",
    "### Low-Rank Approximations\n",
    "\n",
    "__Theorem 8.__ $A$ is the sum of the $r$ rank-one matrices:\n",
    "$$A = \\sum_{j=1}^{r} \\sigma_{j}u_{j}v_{j}^{T}$$\n",
    "\n",
    "__Theorem 9.__ For any $v$ with $0 \\leq v \\leq r$, define\n",
    "$$A = \\sum_{j=1}^{v} \\sigma_{j}u_{j}v_{j}^{T}$$\n",
    "if $v = p = min\\{m,n\\}, define \\sigma_{v+1} = 0.$ Then\n",
    "$$\\|A - A_{v}\\|_{2} = inf_{B \\in \\mathbb{C}^{m \\times n}, rank(B)\\leq v} \\| A-B\\|_{2} = \\sigma_{v+1}$$\n",
    "\n",
    "__Theorem 10.__  For any $v$ with $0 \\leq v \\leq r$, the matrix $A_{v}$ also satisfies\n",
    "$$\\|A - A_{v}\\|_{F} = inf_{B \\in \\mathbb{C}^{m \\times n}, rank(B)\\leq v} \\| A-B\\|_{F} = \\sqrt{\\sigma_{v+1}^{2} + ... + \\sigma_{r}^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((15,40))\n",
    "\n",
    "#H\n",
    "data[2:10,2:4] = 1\n",
    "data[5:7,4:6] = 1\n",
    "data[2:10,6:8] = 1\n",
    "\n",
    "#E\n",
    "data[3:11,10:12] = 1\n",
    "data[3:5,12:16] = 1\n",
    "data[6:8, 12:16] = 1\n",
    "data[9:11, 12:16] = 1\n",
    "\n",
    "#L\n",
    "data[4:12,18:20] = 1\n",
    "data[10:12,20:24] = 1\n",
    "\n",
    "#L\n",
    "data[5:13,26:28] = 1\n",
    "data[11:13,28:32] = 1\n",
    "\n",
    "#0\n",
    "data[6:14,34:36] = 1\n",
    "data[6:8, 36:38] = 1\n",
    "data[12:14, 36:38] = 1\n",
    "data[6:14,38:40] = 1\n",
    "\n",
    "plt.imshow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### understanding effect of each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "u, diag, vt = np.linalg.svd(data, full_matrices=True)\n",
    "for i in range(1, 16):\n",
    "    diag_matrix = np.concatenate((np.zeros((len(diag[:i]) -1),), diag[i-1: i], np.zeros((40-i),)))\n",
    "    reconstruct = np.dot(np.dot(u, np.diag(diag_matrix)[:15,]), vt)\n",
    "    plt.figure()\n",
    "    plt.imshow(reconstruct)\n",
    "    plt.title('{}'.format(i))\n",
    "    plt.xlabel('std orig={}, std reconstruct={}, diff={}'.format(round(np.std(data),3), round(np.std(reconstruct),3), round(np.std(data - reconstruct),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### summing up components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "u, diag, vt = np.linalg.svd(data, full_matrices=True)\n",
    "for i in range(1, 16):\n",
    "    diag_matrix = np.concatenate((diag[:i], np.zeros((40-i),)))\n",
    "    reconstruct = np.dot(np.dot(u, np.diag(diag_matrix)[:15,]), vt)\n",
    "    plt.figure()\n",
    "    plt.imshow(reconstruct)\n",
    "    plt.title('{}'.format(i))\n",
    "    plt.xlabel('std orig={}, std reconstruct={}, diff={}'.format(round(np.std(data),3), round(np.std(reconstruct),3), round(np.std(data - reconstruct),3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
